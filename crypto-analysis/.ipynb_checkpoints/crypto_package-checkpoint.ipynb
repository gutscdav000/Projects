{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, pickle, quandl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import time\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "py.init_notebook_mode(connected = True)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#                        API Functions\n",
    "########################################################################\n",
    "\n",
    "#purpose: download and cache a Quandl dataseries\n",
    "#signature: get_quandle_data(quandl_id: ) -> df: pandas dataframe\n",
    "## *** quandl API key = WZpC82bNrwpdw59PaRK1\n",
    "def get_quandl_data(quandl_id):\n",
    "    home = os.getcwd()\n",
    "    \n",
    "    # create directory if it doesn't already exist\n",
    "    if 'quandl_cache' not in os.listdir(home):\n",
    "        os.mkdir('quandl_cache')\n",
    "    \n",
    "    #change directory to quandl_cache\n",
    "    path = os.path.join(home, 'quandl_cache/')\n",
    "    os.chdir(path)\n",
    "        \n",
    "    # download & cache\n",
    "    cache_path = 'quandl_cache/{}.pkl'.format(quandl_id).replace('/','-')\n",
    "    try:\n",
    "        #serialize python object structure\n",
    "        file = open(cache_path, 'rb')\n",
    "        df = pickle.load(file)\n",
    "        print('Loaded {} from cache'.format(quandl_id))\n",
    "    except (OSError, IOError) as e:\n",
    "        print('Downloading {} frpm Quandl'.format(quandl_id))\n",
    "        df = quandl.get(quandl_id, returns = \"pandas\")\n",
    "        df.to_pickle(cache_path)\n",
    "        print('Cached {} at {}'.format(quandl_id, cache_path))\n",
    "        \n",
    "    os.chdir('../') \n",
    "    return df\n",
    "\n",
    "\n",
    "# purpose: merge common column of each dataframe into combined dataframe\n",
    "# signature: merge_dfs_on_column(dataframes: list, labels: list, col: String) \n",
    "# -> pd.DataFrame(series_dict): dataframe\n",
    "def merge_dfs_on_column(dataframes, labels, col):\n",
    "    series_dict = {labels[i] : dataframes[i][col] for i in range(len(dataframes))}\n",
    "    return pd.DataFrame(series_dict)\n",
    "\n",
    "# purpose: \n",
    "# signature: df_scatter(df:dataframe, title, seperate_y_axis:Boolean, y_axis_label:String, scale:String, initial_hide:boolean) \n",
    "# -> ploty scatter plot\n",
    "def df_scatter(df, title, seperate_y_axis = False, y_axis_label='', scale='linear', initial_hide=False): \n",
    "    label_arr = list(df)\n",
    "    # lambda to be used for form trace\n",
    "    series_arr = list(map(lambda col: df[col], label_arr))\n",
    "    \n",
    "    # plot layout config\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        legend=dict(orientation=\"h\"),\n",
    "        xaxis=dict(type='date'),\n",
    "        yaxis=dict(\n",
    "            title=y_axis_label,\n",
    "            showticklabels= not seperate_y_axis,\n",
    "            type=scale\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    y_axis_config = dict(\n",
    "        overlaying='y',\n",
    "        showticklabels=False,\n",
    "        type=scale )\n",
    "    \n",
    "    visibility = 'visible'\n",
    "    if initial_hide:\n",
    "        visibility = 'legendonly'\n",
    "        \n",
    "    # Form Trace for each series\n",
    "    trace_arr = []\n",
    "    for index, series in enumerate(series_arr):\n",
    "        trace = go.Scatter(\n",
    "            x=series.index, \n",
    "            y=series, \n",
    "            name=label_arr[index],\n",
    "            visible=visibility\n",
    "        )\n",
    "        \n",
    "        # add separate axis for series \n",
    "        if seperate_y_axis:\n",
    "            trace['yaxis'] = 'y{}'.format(index + 1)\n",
    "            layout['yaxis{}'.format(index + 1)] = y_axis_config  \n",
    "        \n",
    "        trace_arr.append(trace)\n",
    "    \n",
    "    fig = go.Figure(data = trace_arr, layout = layout)\n",
    "    py.iplot(fig)\n",
    "    \n",
    "# purpose: Download and cache JSON data, return as a dataframe.\n",
    "# signature: get_json_data(json_url:String, cache_path:String)\n",
    "# -> df: dataframe\n",
    "def get_json_data(json_url, cache_path): \n",
    "    try:        \n",
    "        f = open(cache_path, 'rb')\n",
    "        df = pickle.load(f)   \n",
    "        print('Loaded {} from cache'.format(json_url))\n",
    "    except (OSError, IOError) as e:\n",
    "        print('Downloading {}'.format(json_url))\n",
    "        df = pd.read_json(json_url)\n",
    "        df.to_pickle(cache_path)\n",
    "        print('Cached {} at {}'.format(json_url, cache_path))\n",
    "    return df\n",
    "    \n",
    "    \n",
    "#purpose: retrieve crypto data from poloniex \n",
    "#signiture: get_crypto_data(poloniex_pair:String) -> data_df: dataframe\n",
    "def get_crypto_data(poloniex_pair):\n",
    "    BASE_POLO_URL = 'https://poloniex.com/public?command=returnChartData&currencyPair={}&start={}&end={}&period={}'\n",
    "    start_date = datetime.strptime('2015-01-01', '%Y-%m-%d') # get data from start of 2015\n",
    "    end_date = datetime.now()\n",
    "    period = 86400 # pull daily data (86,400 seconds per day)\n",
    "    \n",
    "    # use time() instead of timestamp() with python 2\n",
    "    json_url = BASE_POLO_URL.format(poloniex_pair, start_date.time(), end_date.time(), period)\n",
    "    #json_url = BASE_POLO_URL.format(poloniex_pair, start_date.timestamp(), end_date.timestamp(), period)\n",
    "    #json_url = BASE_POLO_URL.format(poloniex_pair, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), period)\n",
    "    \n",
    "    data_df = get_json_data(json_url, poloniex_pair)\n",
    "    data_df = data_df.set_index('date')\n",
    "    return data_df\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# a function which takes two numpy arrays of equal size and calculates\n",
    "# the pearson correlation coefficient for the two sets of data\n",
    "# @params 2 numpy arrays\n",
    "# @returns a float corresponding to the pearson r value\n",
    "def pearson_coefficient(npX, npY):\n",
    "    if len(npX) != len(npY):\n",
    "        return -23\n",
    "    \n",
    "    n = len(npX)\n",
    "    x2 = 0\n",
    "    y2 = 0\n",
    "    x = 0\n",
    "    y = 0\n",
    "    xy = 0\n",
    "    \n",
    "    #sum coefficient parameters\n",
    "    for i in range(n):\n",
    "        x2 += npX[i] ** 2\n",
    "        y2 += npY[i] ** 2\n",
    "        x += npX[i]\n",
    "        y += npY[i]\n",
    "        xy += npX[i] * npY[i]\n",
    "\n",
    "    coef = float((xy - ((x * y)/n)) /math.sqrt((x2 - ((x ** 2) / n)) * (y2 - ((y ** 2) / n))))\n",
    "    \n",
    "    return coef\n",
    "\n",
    "\n",
    "#purpose: This is a new function for correlating two crypto coins via a plot\n",
    "# crypto_correlation(pandas.DataFrame, string, pandas.DataFrame, string) \n",
    "# --> *returns: pearson coeff *prints: Plot\n",
    "def crypto_correlation(df_left, name1, df_right, name2, plotBool=False):\n",
    "    #make sure dates match && read df into arrays\n",
    "    left_returns, right_returns = match_dates(df_left, name1, df_right, name2)\n",
    "    print(str(len(left_returns)),str(len(right_returns)))\n",
    "    \n",
    "    # calculate values for linear regression\n",
    "    slope, intercept, r_val, p_val, std_err = stats.linregress(left_returns, right_returns)\n",
    "    \n",
    "    best_fit_x = np.arange(min(left_returns), max(left_returns), (max(left_returns)- min(left_returns)) / 10000.0)#not sure this is right for my model?\n",
    "    print(\"slope:\", slope, \"\\tintercept:\", intercept)\n",
    "    best_fit_y = slope * best_fit_x + intercept\n",
    "    plot = [go.Scatter(\n",
    "        x = left_returns,\n",
    "        y = right_returns,\n",
    "        name = str(name1).title() + \"vs. \" + str(name2).title() + \"Correlation\",\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 10,\n",
    "            color = 'rgba(255, 0, 0, .9)',\n",
    "            line = dict(width = 2,)\n",
    "        )\n",
    "    ), go.Scatter(x = best_fit_x,\n",
    "                  y = best_fit_y,\n",
    "                  mode='lines',\n",
    "                  marker=go.Marker(color='rgb(31, 119, 180)'),\n",
    "                  name='Fit'\n",
    "              )]\n",
    "    layout = dict( \n",
    "        title = str(name1)+ ' vs. '+str(name2)+' correlation',\n",
    "        #yaxis = dict(zeroline = False),\n",
    "        #xaxis = dict(zeroline = False)\n",
    "    )\n",
    "          \n",
    "    if plotBool:\n",
    "       fig = dict(data=plot,layout=layout)\n",
    "       py.iplot(fig, filename='plot')\n",
    "    \n",
    "    return pearson_coefficient(left_returns, right_returns)\n",
    "\n",
    "\n",
    "\n",
    "#(HELPER FOR crypto_correlation function)date match function\n",
    "# purpose: returns 2 numpy arrays of daily returns for corresponding days of 2 different currencies\n",
    "# match_dates(left_df: dataframe, right_df: dataframe) -> np.Array, np.Array\n",
    "def match_dates(left_df, symbol1, right_df, symbol2):\n",
    "    \n",
    "    # merge dataframes on the date (e.g. the index of the df)\n",
    "    merged_df = pd.merge(left_df, right_df, how = 'inner', left_index=True, right_index=True, validate = \"one_to_one\")\n",
    "    #######################################################################\n",
    "    # so it's merging on date, but I need to differentiate between prices of each currency\n",
    "    #######################################################################\n",
    "    \n",
    "    lst1 = []\n",
    "    lst2 = []\n",
    "    \n",
    "    #actually range of merged \n",
    "    range_df = range(len(merged_df) - 1)\n",
    "\n",
    "    # calculate daily returns\n",
    "    for i in range_df:\n",
    "        lst1.append((merged_df['price_usd_x'][i + 1] - merged_df['price_usd_x'][i]) / merged_df['price_usd_x'][i])\n",
    "        #print(\"i:\", i, \"\\trange\\[-1\\]\",range_df[-1])\n",
    "        #if i < range_df[-1]:\n",
    "        lst2.append((merged_df['price_usd_y'][i + 1] - merged_df['price_usd_y'][i]) / merged_df['price_usd_y'][i])\n",
    "        \n",
    "    # return 2 numpy arrays containing daily returns for matching dates\n",
    "    npA1 = np.array(lst1)\n",
    "    npA2 = np.array(lst2)\n",
    "\n",
    "    return npA1, npA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a28d618e1b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#change dir to poloniex_cache & create dir if it doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'poloniex_cache'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'poloniex_cache'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os, pickle, quandl, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "# compile data\n",
    "########################################################\n",
    "\n",
    "# exchange info\n",
    "altcoins = ['ETH','LTC','XRP','ETC','STR','DASH','XMR','XEM', 'GNT']\n",
    "alt_data = {}\n",
    "\n",
    "#change dir to poloniex_cache & create dir if it doesn't exist \n",
    "if not 'poloniex_cache' in os.listdir(os.getcwd()):\n",
    "    os.mkdir('poloniex_cache')\n",
    "    \n",
    "path = os.path.join(os.getcwd(), 'poloniex_cache')\n",
    "os.chdir(path)\n",
    "\n",
    "for alt in altcoins:\n",
    "    coinpair = 'BTC_{}'.format(alt)\n",
    "    crypto_price_df = CP.get_crypto_data(coinpair)\n",
    "    alt_data[alt] = crypto_price_df\n",
    "\n",
    "#leave the directory\n",
    "os.chdir('../')\n",
    "\n",
    "exchanges = ['KRAKEN','COINBASE', 'BITSTAMP','ITBIT','OKCOIN', 'GETBTC']#,'COINSBANK','HITBTC','LYBIT','ANXHK','BITME','BITBOX','INTRSNG','BTCE','WEEX','JUST','CBX']\n",
    "exch_data = {}\n",
    "# retrieve exchange data and read into dictionary\n",
    "for exchange in exchanges:\n",
    "    exchange_df = CP.get_quandl_data('BCHARTS/{}USD'.format(exchange))\n",
    "    #time.sleep(1000)\n",
    "    exch_data[exchange] = exchange_df\n",
    "    \n",
    "# Merge BTC price data series' into single dataframe\n",
    "btc_usd_datasets = CP.merge_dfs_on_column(list(exch_data.values()), list(exch_data.keys()), 'Weighted Price')\n",
    "btc_usd_datasets.replace(0, np.nan, inplace = True) # remove 0 values\n",
    "# Calc avg in new column\n",
    "btc_usd_datasets['avg_btc_price_usd'] = btc_usd_datasets.mean(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# calculate USD Price as new col in each dataframe\n",
    "for alt in alt_data.keys():\n",
    "    alt_data[alt]['price_usd'] = alt_data[alt]['weightedAverage'] * btc_usd_datasets['avg_btc_price_usd']\n",
    "    \n",
    "\n",
    "########################################################\n",
    "# organize into daily returns\n",
    "########################################################\n",
    "dailyReturns = {}\n",
    "\n",
    "for alt in alt_data.keys():\n",
    "    dailyReturns[alt] = np.empty(len(alt_data[alt]['price_usd']) - 1)\n",
    "    for i in range(len(alt_data[alt]['price_usd']) - 1):\n",
    "        # calculate returns for day\n",
    "        dayRet = (alt_data[alt]['price_usd'][i + 1] - alt_data[alt]['price_usd'][i]) / alt_data[alt]['price_usd'][i]\n",
    "        #insert them into numpy array\n",
    "        np.insert(dailyReturns[alt], i, dayRet)\n",
    "\n",
    "        \n",
    "#matrix to contain preprocessed pearson correlation values between currencies\n",
    "pearsonValueMatrix = np.empty([len(alt_data.keys()), len(alt_data.keys())])\n",
    "indexVector = []\n",
    "i = 0\n",
    "j = 0\n",
    "for a1 in alt_data.keys():\n",
    "    indexVector.append(i)\n",
    "    for a2 in alt_data.keys():\n",
    "        np.insert(pearsonValueMatrix, [], CP.pearson_coefficient(dailyReturns[a1], dailyReturns[a2]))\n",
    "        j += 1\n",
    "    i += 1\n",
    "    \n",
    "print(pearsonValueMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
